{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping from DARAZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting review: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".content\"}\n",
      "  (Session info: chrome=133.0.6943.142); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7AF0CC6A5+28789]\n",
      "\t(No symbol) [0x00007FF7AF035B20]\n",
      "\t(No symbol) [0x00007FF7AEEC8F9A]\n",
      "\t(No symbol) [0x00007FF7AEF1F346]\n",
      "\t(No symbol) [0x00007FF7AEF1F57C]\n",
      "\t(No symbol) [0x00007FF7AEF11CFC]\n",
      "\t(No symbol) [0x00007FF7AEF4736F]\n",
      "\t(No symbol) [0x00007FF7AEF11BC6]\n",
      "\t(No symbol) [0x00007FF7AEF47540]\n",
      "\t(No symbol) [0x00007FF7AEF6F7E3]\n",
      "\t(No symbol) [0x00007FF7AEF47103]\n",
      "\t(No symbol) [0x00007FF7AEF0FFC0]\n",
      "\t(No symbol) [0x00007FF7AEF11273]\n",
      "\tGetHandleVerifier [0x00007FF7AF411AED+3458237]\n",
      "\tGetHandleVerifier [0x00007FF7AF42829C+3550316]\n",
      "\tGetHandleVerifier [0x00007FF7AF41DB9D+3507565]\n",
      "\tGetHandleVerifier [0x00007FF7AF192C6A+841274]\n",
      "\t(No symbol) [0x00007FF7AF0409EF]\n",
      "\t(No symbol) [0x00007FF7AF03CB34]\n",
      "\t(No symbol) [0x00007FF7AF03CCD6]\n",
      "\t(No symbol) [0x00007FF7AF02C119]\n",
      "\tBaseThreadInitThunk [0x00007FFF6AD7259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFF6D0CAF38+40]\n",
      "\n",
      "Reviews saved to 'daraz_reviews.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Disable automation flag\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the Daraz product page\n",
    "url = 'https://www.daraz.lk/products/swisstek-multi-purpose-ladder-20ft-i113288341-s1024286031.html'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to load reviews (if needed)\n",
    "for _ in range(3):  # Adjust the number of scrolls based on the number of reviews\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# Wait for the reviews section to load\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'item-content'))  # Wait for reviews to load\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Reviews section not found:\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Find the reviews section\n",
    "reviews = driver.find_elements(By.CLASS_NAME, 'item-content')  # Update class name if needed\n",
    "\n",
    "# Extract review text (comments) only\n",
    "review_data = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        review_text = review.find_element(By.CLASS_NAME, 'content').text.strip()  # Extract review text\n",
    "        review_data.append({\n",
    "            'review_text': review_text\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # Skip reviews with missing data\n",
    "        print(f\"Error extracting review: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(review_data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('../Data/webscraped/daraz/reviews.csv', index=False)\n",
    "print(\"Reviews saved to 'daraz_reviews.csv'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data webscrape from the daraz directly and some are entered manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully created!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Combining seperate csv files into one file\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder where your CSV files are stored\n",
    "folder_path = \"../Data/webscraped\"  #('../data/webscrape_reviews/reviews.csv', index=False)\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file and process\n",
    "for file in csv_files:\n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract product type and size from filename (assuming format \"productType_size.csv\")\n",
    "    product_type, size = file.replace(\".csv\", \"\").split(\"_\")\n",
    "\n",
    "    # Add new columns\n",
    "    df[\"product_type\"] = product_type\n",
    "    df[\"size\"] = size\n",
    "\n",
    "    # Append to list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Add a unique ID column\n",
    "final_df[\"ID\"] = range(1, len(final_df) + 1)\n",
    "\n",
    "# Select the required columns\n",
    "final_df = final_df[[\"ID\", \"product_type\", \"size\",\"review_text\",\"rating\"]]\n",
    "\n",
    "\n",
    "# Save to a new CSV file\n",
    "final_df.to_csv(\"../Data/webscraped/combined_reviews.csv\", index=False)\n",
    "\n",
    "print(\"Dataset successfully created!\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining dataset and translate using Google Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dataset successfully created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from googletrans import Translator\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "def translate_text(text):\n",
    "    \"\"\"Detect language and translate Sinhala text into English.\"\"\"\n",
    "    try:\n",
    "        detected_lang = translator.detect(text).lang\n",
    "        if detected_lang == \"si\":  # Sinhala language code\n",
    "            return translator.translate(text, src=\"si\", dest=\"en\").text\n",
    "        return text  # Keep English text unchanged\n",
    "    except:\n",
    "        return text  # Return original text if error occurs\n",
    "\n",
    "# Define folder path\n",
    "folder_path = \"../Data/webscraped\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file and process\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract product type and size from filename\n",
    "    product_type, size = file.replace(\".csv\", \"\").split(\"_\")\n",
    "\n",
    "    # Add new columns\n",
    "    df[\"product_type\"] = product_type\n",
    "    df[\"size\"] = size\n",
    "\n",
    "    # Translate review_text\n",
    "    df[\"translated_review\"] = df[\"review_text\"].astype(str).apply(translate_text)\n",
    "\n",
    "    # Append to list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Add a unique ID column\n",
    "final_df[\"ID\"] = range(1, len(final_df) + 1)\n",
    "\n",
    "# Select final columns\n",
    "final_df = final_df[[\"ID\", \"product_type\", \"size\", \"review_text\", \"translated_review\", \"rating\"]]\n",
    "\n",
    "# Save to new CSV file\n",
    "final_df.to_csv(\"../Data/google_API/translated_reviews.csv\", index=False)\n",
    "\n",
    "print(\"Translated dataset successfully created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import MarianMTModel, MarianTokenizer\\n\\n# Load pre-trained translation model (Sinhala → English)\\nmodel_name = \"Helsinki-NLP/opus-mt-si-en\"\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\nmodel = MarianMTModel.from_pretrained(model_name)\\n\\ndef translate_sinhala(text):\\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\\n    translated_tokens = model.generate(**inputs)\\n    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\\n\\n# Apply translation to Sinhala reviews\\nfinal_df[\"translated_review\"] = final_df[\"review_text\"].apply(translate_sinhala)\\n\\n# Save the dataset\\nfinal_df.to_csv(\"translated_reviews.csv\", index=False)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained translation model (Sinhala → English)\n",
    "model_name = \"Helsinki-NLP/opus-mt-si-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate_sinhala(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply translation to Sinhala reviews\n",
    "final_df[\"../Data/google_API/translated_review\"] = final_df[\"review_text\"].apply(translate_sinhala)\n",
    "\n",
    "# Save the dataset\n",
    "final_df.to_csv(\"translated_reviews.csv\", index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review dataset successfully created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the translated dataset\n",
    "df = pd.read_csv(\"../Data/google_API/translated_reviews.csv\")\n",
    "\n",
    "# Rename the translated column to 'text'\n",
    "df.rename(columns={\"translated_review\": \"text\"}, inplace=True)\n",
    "\n",
    "# Select required columns\n",
    "final_df = df[[\"ID\", \"product_type\", \"size\", \"text\", \"rating\"]]\n",
    "\n",
    "# Save the new CSV file\n",
    "final_df.to_csv(\"../Data/google_API/reviews.csv\", index=False)\n",
    "\n",
    "print(\"Review dataset successfully created!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
